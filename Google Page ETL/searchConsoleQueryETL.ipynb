{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import boto3\n",
    "from google.oauth2 import service_account\n",
    "from googleapiclient.discovery import build\n",
    "import snowflake.connector\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "CLIENT = 'OCF'\n",
    "# Function to install packages\n",
    "def install_packages():\n",
    "    try:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-r\", \"requirements.txt\"])\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to install packages from requirements.txt: {e}\")\n",
    "        sys.exit(1)\n",
    "install_packages()\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Constants and credentials\n",
    "SITE_URL = os.getenv(\"SITE_URL\")\n",
    "BUCKET = os.getenv(\"BUCKET\")\n",
    "FOLDER = os.getenv(\"FOLDER\")\n",
    "SNOWFLAKE_TABLE = os.getenv(\"SNOWFLAKE_TABLE\")\n",
    "STAGING_TABLE = os.getenv(\"STAGING_TABLE\")\n",
    "BATCH_SIZE = int(os.getenv(\"BATCH_SIZE\", \"100000\"))\n",
    "\n",
    "GOOGLE_CREDENTIALS_PATH = os.getenv(\"GOOGLE_APPLICATION_CREDENTIALS\")\n",
    "AWS_ACCESS_KEY_ID = os.getenv(\"AWS_ACCESS_KEY_ID\")\n",
    "AWS_SECRET_ACCESS_KEY = os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    "SNOWFLAKE_USER = os.getenv(\"SNOWFLAKE_USER\")\n",
    "SNOWFLAKE_PASSWORD = os.getenv(\"SNOWFLAKE_PASSWORD\")\n",
    "SNOWFLAKE_ACCOUNT = os.getenv(\"SNOWFLAKE_ACCOUNT\")\n",
    "SNOWFLAKE_WAREHOUSE = os.getenv(\"SNOWFLAKE_WAREHOUSE\")\n",
    "SNOWFLAKE_DATABASE = os.getenv(\"SNOWFLAKE_DATABASE\")\n",
    "SNOWFLAKE_SCHEMA = os.getenv(\"SNOWFLAKE_SCHEMA\")\n",
    "SNOWFLAKE_ROLE = os.getenv(\"SNOWFLAKE_ROLE\")\n",
    "\n",
    "client = 'ocf'\n",
    "# Initialize S3 client\n",
    "s3 = boto3.client(\n",
    "    's3',\n",
    "    aws_access_key_id=AWS_ACCESS_KEY_ID,\n",
    "    aws_secret_access_key=AWS_SECRET_ACCESS_KEY,\n",
    "    region_name='us-east-1'\n",
    ")\n",
    "\n",
    "# Connect to Snowflake\n",
    "def connect_snowflake():\n",
    "    if not SNOWFLAKE_ACCOUNT or not SNOWFLAKE_USER:\n",
    "        raise ValueError(\"Snowflake credentials are missing.\")\n",
    "    conn = snowflake.connector.connect(\n",
    "        user=SNOWFLAKE_USER,\n",
    "        password=SNOWFLAKE_PASSWORD,\n",
    "        account=SNOWFLAKE_ACCOUNT,\n",
    "        warehouse=SNOWFLAKE_WAREHOUSE,\n",
    "        database=SNOWFLAKE_DATABASE,\n",
    "        schema=SNOWFLAKE_SCHEMA,\n",
    "        role=SNOWFLAKE_ROLE\n",
    "    )\n",
    "    print(\"Connected to Snowflake.\")\n",
    "    return conn\n",
    "\n",
    "# Authenticate Google Search Console API\n",
    "def auth_search_console():\n",
    "    credentials = service_account.Credentials.from_service_account_file(\n",
    "        GOOGLE_CREDENTIALS_PATH,\n",
    "        scopes=[\"https://www.googleapis.com/auth/webmasters.readonly\"]\n",
    "    )\n",
    "    return build(\"searchconsole\", \"v1\", credentials=credentials)\n",
    "\n",
    "# Fetch data from Google Search Console\n",
    "def fetch_data_from_gsc(service, start_date, end_date):\n",
    "    request = {\n",
    "        'startDate': start_date,\n",
    "        'endDate': end_date,\n",
    "        'dimensions': ['page']\n",
    "    }\n",
    "    response = service.searchanalytics().query(siteUrl=SITE_URL, body=request).execute()\n",
    "    if 'rows' in response:\n",
    "        data = [\n",
    "            {\n",
    "                'Page': row['keys'][0],\n",
    "                'Clicks': row.get('clicks', 0),\n",
    "                'Impressions': row.get('impressions', 0),\n",
    "                'CTR': row.get('ctr', 0),\n",
    "                'Position': row.get('position', 0)\n",
    "            }\n",
    "            for row in response['rows']\n",
    "        ]\n",
    "        df = pd.DataFrame(data)\n",
    "        df['start_dt'] = start_date\n",
    "        df['end_dt'] = end_date\n",
    "        return df\n",
    "    else:\n",
    "        print(\"No data found.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "# Upload data to S3 in batches\n",
    "def upload_to_s3(df):\n",
    "    num_batches = len(df) // BATCH_SIZE + 1\n",
    "    for i in range(num_batches):\n",
    "        start_idx = i * BATCH_SIZE\n",
    "        end_idx = min((i + 1) * BATCH_SIZE, len(df))\n",
    "        batch_data = df[start_idx:end_idx]\n",
    "        file_name = f'batch_{i+1}.csv'\n",
    "        batch_data.to_csv(file_name, index=False)\n",
    "        batch_data['client'] = CLIENT\n",
    "        s3.upload_file(file_name, BUCKET, f\"{FOLDER}/{file_name}\")\n",
    "        os.remove(file_name)\n",
    "        print(f\"Uploaded {file_name} to S3.\")\n",
    "\n",
    "# Load data from S3 to Snowflake\n",
    "def load_to_snowflake(conn):\n",
    "    cursor = conn.cursor()\n",
    "    try:\n",
    "        copy_command = f\"\"\"\n",
    "        COPY INTO {STAGING_TABLE}\n",
    "        FROM 's3://{BUCKET}/{FOLDER}/'\n",
    "        CREDENTIALS = (AWS_KEY_ID='{AWS_ACCESS_KEY_ID}' AWS_SECRET_KEY='{AWS_SECRET_ACCESS_KEY}')\n",
    "        FILE_FORMAT = (TYPE = 'CSV' FIELD_OPTIONALLY_ENCLOSED_BY = '\"' SKIP_HEADER=1)\n",
    "        ON_ERROR='CONTINUE';\n",
    "        \"\"\"\n",
    "        cursor.execute(copy_command)\n",
    "        print(\"Data copied into staging table.\")\n",
    "\n",
    "        merge_command = f\"\"\"MERGE INTO {SNOWFLAKE_TABLE} AS tgt\n",
    "        USING {STAGING_TABLE} AS src\n",
    "        ON tgt.PAGE = src.PAGE\n",
    "        AND tgt.START_DT = src.START_DT\n",
    "        AND tgt.END_DT = src.END_DT\n",
    "        AND tgt.COMPANY = src.COMPANY\n",
    "        WHEN MATCHED THEN\n",
    "            UPDATE SET\n",
    "                tgt.CLICKS = src.CLICKS,\n",
    "                tgt.IMPRESSIONS = src.IMPRESSIONS,\n",
    "                tgt.CTR = src.CTR,\n",
    "                tgt.POSITION = src.POSITION\n",
    "        WHEN NOT MATCHED THEN\n",
    "            INSERT (PAGE, CLICKS, IMPRESSIONS, CTR, POSITION, START_DT, END_DT,COMPANY , COMPANY_ID)\n",
    "            VALUES (src.PAGE, src.CLICKS, src.IMPRESSIONS, src.CTR, src.POSITION, src.START_DT, src.END_DT,src.COMPANY , src.COMPANY_ID);\n",
    "        \"\"\"\n",
    "\n",
    "        cursor.execute(merge_command)\n",
    "        cursor.execute(f\"truncate {STAGING_TABLE};\")\n",
    "\n",
    "        print(\"Data merged into target table.\")\n",
    "    finally:\n",
    "        cursor.close()\n",
    "\n",
    "# Clear S3 folder\n",
    "def clear_s3_folder():\n",
    "    response = s3.list_objects_v2(Bucket=BUCKET, Prefix=FOLDER)\n",
    "    if 'Contents' in response:\n",
    "        for obj in response['Contents']:\n",
    "            s3.delete_object(Bucket=BUCKET, Key=obj['Key'])\n",
    "            print(f\"Deleted {obj['Key']} from S3.\")\n",
    "    else:\n",
    "        print(\"No objects found to delete.\")\n",
    "\n",
    "# Main ETL Process\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Snowflake.\n",
      "Processing data for date: 2024-09-13\n",
      "No data found.\n",
      "No data to process for date: 2024-09-13.\n",
      "Processing data for date: 2024-09-14\n",
      "No data found.\n",
      "No data to process for date: 2024-09-14.\n",
      "Processing data for date: 2024-09-15\n",
      "No data found.\n",
      "No data to process for date: 2024-09-15.\n",
      "Processing data for date: 2024-09-16\n",
      "No data found.\n",
      "No data to process for date: 2024-09-16.\n",
      "Processing data for date: 2024-09-17\n",
      "No data found.\n",
      "No data to process for date: 2024-09-17.\n",
      "Processing data for date: 2024-09-18\n",
      "No data found.\n",
      "No data to process for date: 2024-09-18.\n",
      "Processing data for date: 2024-09-19\n",
      "No data found.\n",
      "No data to process for date: 2024-09-19.\n",
      "Processing data for date: 2024-09-20\n",
      "No data found.\n",
      "No data to process for date: 2024-09-20.\n",
      "Processing data for date: 2024-09-21\n",
      "Uploaded batch_1.csv to S3.\n",
      "Data copied into staging table.\n",
      "Data merged into target table.\n",
      "Deleted Query/batch_1.csv from S3.\n",
      "ETL process completed for date: 2024-09-21.\n",
      "Processing data for date: 2024-09-22\n",
      "Uploaded batch_1.csv to S3.\n",
      "Data copied into staging table.\n",
      "Data merged into target table.\n",
      "Deleted Query/batch_1.csv from S3.\n",
      "ETL process completed for date: 2024-09-22.\n",
      "Processing data for date: 2024-09-23\n",
      "Uploaded batch_1.csv to S3.\n",
      "Data copied into staging table.\n",
      "Data merged into target table.\n",
      "Deleted Query/batch_1.csv from S3.\n",
      "ETL process completed for date: 2024-09-23.\n",
      "Processing data for date: 2024-09-24\n",
      "Uploaded batch_1.csv to S3.\n",
      "Data copied into staging table.\n",
      "Data merged into target table.\n",
      "Deleted Query/batch_1.csv from S3.\n",
      "ETL process completed for date: 2024-09-24.\n",
      "Processing data for date: 2024-09-25\n",
      "Uploaded batch_1.csv to S3.\n",
      "Data copied into staging table.\n",
      "Data merged into target table.\n",
      "Deleted Query/batch_1.csv from S3.\n",
      "ETL process completed for date: 2024-09-25.\n",
      "Processing data for date: 2024-09-26\n",
      "Uploaded batch_1.csv to S3.\n",
      "Data copied into staging table.\n",
      "Data merged into target table.\n",
      "Deleted Query/batch_1.csv from S3.\n",
      "ETL process completed for date: 2024-09-26.\n",
      "Processing data for date: 2024-09-27\n",
      "Uploaded batch_1.csv to S3.\n",
      "Data copied into staging table.\n",
      "Data merged into target table.\n",
      "Deleted Query/batch_1.csv from S3.\n",
      "ETL process completed for date: 2024-09-27.\n",
      "Processing data for date: 2024-09-28\n",
      "Uploaded batch_1.csv to S3.\n",
      "Data copied into staging table.\n",
      "Data merged into target table.\n",
      "Deleted Query/batch_1.csv from S3.\n",
      "ETL process completed for date: 2024-09-28.\n",
      "Processing data for date: 2024-09-29\n",
      "Uploaded batch_1.csv to S3.\n",
      "Data copied into staging table.\n",
      "Data merged into target table.\n",
      "Deleted Query/batch_1.csv from S3.\n",
      "ETL process completed for date: 2024-09-29.\n",
      "Processing data for date: 2024-09-30\n",
      "Uploaded batch_1.csv to S3.\n",
      "Data copied into staging table.\n",
      "Data merged into target table.\n",
      "Deleted Query/batch_1.csv from S3.\n",
      "ETL process completed for date: 2024-09-30.\n",
      "Processing data for date: 2024-10-01\n",
      "Uploaded batch_1.csv to S3.\n",
      "Data copied into staging table.\n",
      "Data merged into target table.\n",
      "Deleted Query/batch_1.csv from S3.\n",
      "ETL process completed for date: 2024-10-01.\n",
      "Processing data for date: 2024-10-02\n",
      "Uploaded batch_1.csv to S3.\n",
      "Data copied into staging table.\n",
      "Data merged into target table.\n",
      "Deleted Query/batch_1.csv from S3.\n",
      "ETL process completed for date: 2024-10-02.\n",
      "Processing data for date: 2024-10-03\n",
      "Uploaded batch_1.csv to S3.\n",
      "Data copied into staging table.\n",
      "Data merged into target table.\n",
      "Deleted Query/batch_1.csv from S3.\n",
      "ETL process completed for date: 2024-10-03.\n",
      "Processing data for date: 2024-10-04\n",
      "Uploaded batch_1.csv to S3.\n",
      "Data copied into staging table.\n",
      "Data merged into target table.\n",
      "Deleted Query/batch_1.csv from S3.\n",
      "ETL process completed for date: 2024-10-04.\n",
      "Processing data for date: 2024-10-05\n",
      "Uploaded batch_1.csv to S3.\n",
      "Data copied into staging table.\n",
      "Data merged into target table.\n",
      "Deleted Query/batch_1.csv from S3.\n",
      "ETL process completed for date: 2024-10-05.\n",
      "Processing data for date: 2024-10-06\n",
      "Uploaded batch_1.csv to S3.\n",
      "Data copied into staging table.\n",
      "Data merged into target table.\n",
      "Deleted Query/batch_1.csv from S3.\n",
      "ETL process completed for date: 2024-10-06.\n",
      "Processing data for date: 2024-10-07\n",
      "Uploaded batch_1.csv to S3.\n",
      "Data copied into staging table.\n",
      "Data merged into target table.\n",
      "Deleted Query/batch_1.csv from S3.\n",
      "ETL process completed for date: 2024-10-07.\n",
      "Processing data for date: 2024-10-08\n",
      "Uploaded batch_1.csv to S3.\n",
      "Data copied into staging table.\n",
      "Data merged into target table.\n",
      "Deleted Query/batch_1.csv from S3.\n",
      "ETL process completed for date: 2024-10-08.\n",
      "Processing data for date: 2024-10-09\n",
      "Uploaded batch_1.csv to S3.\n",
      "Data copied into staging table.\n",
      "Data merged into target table.\n",
      "Deleted Query/batch_1.csv from S3.\n",
      "ETL process completed for date: 2024-10-09.\n",
      "Processing data for date: 2024-10-10\n",
      "Uploaded batch_1.csv to S3.\n",
      "Data copied into staging table.\n",
      "Data merged into target table.\n",
      "Deleted Query/batch_1.csv from S3.\n",
      "ETL process completed for date: 2024-10-10.\n",
      "Processing data for date: 2024-10-11\n",
      "Uploaded batch_1.csv to S3.\n",
      "Data copied into staging table.\n",
      "Data merged into target table.\n",
      "Deleted Query/batch_1.csv from S3.\n",
      "ETL process completed for date: 2024-10-11.\n",
      "Processing data for date: 2024-10-12\n",
      "Uploaded batch_1.csv to S3.\n",
      "Data copied into staging table.\n",
      "Data merged into target table.\n",
      "Deleted Query/batch_1.csv from S3.\n",
      "ETL process completed for date: 2024-10-12.\n",
      "Processing data for date: 2024-10-13\n",
      "Uploaded batch_1.csv to S3.\n",
      "Data copied into staging table.\n",
      "Data merged into target table.\n",
      "Deleted Query/batch_1.csv from S3.\n",
      "ETL process completed for date: 2024-10-13.\n",
      "Processing data for date: 2024-10-14\n",
      "Uploaded batch_1.csv to S3.\n",
      "Data copied into staging table.\n",
      "Data merged into target table.\n",
      "Deleted Query/batch_1.csv from S3.\n",
      "ETL process completed for date: 2024-10-14.\n",
      "Processing data for date: 2024-10-15\n",
      "Uploaded batch_1.csv to S3.\n",
      "Data copied into staging table.\n",
      "Data merged into target table.\n",
      "Deleted Query/batch_1.csv from S3.\n",
      "ETL process completed for date: 2024-10-15.\n",
      "Processing data for date: 2024-10-16\n",
      "Uploaded batch_1.csv to S3.\n",
      "Data copied into staging table.\n",
      "Data merged into target table.\n",
      "Deleted Query/batch_1.csv from S3.\n",
      "ETL process completed for date: 2024-10-16.\n",
      "Processing data for date: 2024-10-17\n",
      "Uploaded batch_1.csv to S3.\n",
      "Data copied into staging table.\n",
      "Data merged into target table.\n",
      "Deleted Query/batch_1.csv from S3.\n",
      "ETL process completed for date: 2024-10-17.\n",
      "Processing data for date: 2024-10-18\n",
      "Uploaded batch_1.csv to S3.\n",
      "Data copied into staging table.\n",
      "Data merged into target table.\n",
      "Deleted Query/batch_1.csv from S3.\n",
      "ETL process completed for date: 2024-10-18.\n",
      "Processing data for date: 2024-10-19\n",
      "Uploaded batch_1.csv to S3.\n",
      "Data copied into staging table.\n",
      "Data merged into target table.\n",
      "Deleted Query/batch_1.csv from S3.\n",
      "ETL process completed for date: 2024-10-19.\n",
      "Processing data for date: 2024-10-20\n",
      "Uploaded batch_1.csv to S3.\n",
      "Data copied into staging table.\n",
      "Data merged into target table.\n",
      "Deleted Query/batch_1.csv from S3.\n",
      "ETL process completed for date: 2024-10-20.\n",
      "Processing data for date: 2024-10-21\n",
      "Uploaded batch_1.csv to S3.\n",
      "Data copied into staging table.\n",
      "Data merged into target table.\n",
      "Deleted Query/batch_1.csv from S3.\n",
      "ETL process completed for date: 2024-10-21.\n",
      "Processing data for date: 2024-10-22\n",
      "Uploaded batch_1.csv to S3.\n",
      "Data copied into staging table.\n",
      "Data merged into target table.\n",
      "Deleted Query/batch_1.csv from S3.\n",
      "ETL process completed for date: 2024-10-22.\n",
      "Processing data for date: 2024-10-23\n",
      "Uploaded batch_1.csv to S3.\n",
      "Data copied into staging table.\n",
      "Data merged into target table.\n",
      "Deleted Query/batch_1.csv from S3.\n",
      "ETL process completed for date: 2024-10-23.\n",
      "Processing data for date: 2024-10-24\n",
      "Uploaded batch_1.csv to S3.\n",
      "Data copied into staging table.\n",
      "Data merged into target table.\n",
      "Deleted Query/batch_1.csv from S3.\n",
      "ETL process completed for date: 2024-10-24.\n",
      "Processing data for date: 2024-10-25\n",
      "Uploaded batch_1.csv to S3.\n",
      "Data copied into staging table.\n",
      "Data merged into target table.\n",
      "Deleted Query/batch_1.csv from S3.\n",
      "ETL process completed for date: 2024-10-25.\n",
      "Processing data for date: 2024-10-26\n",
      "Uploaded batch_1.csv to S3.\n",
      "Data copied into staging table.\n",
      "Data merged into target table.\n",
      "Deleted Query/batch_1.csv from S3.\n",
      "ETL process completed for date: 2024-10-26.\n",
      "Processing data for date: 2024-10-27\n",
      "Uploaded batch_1.csv to S3.\n",
      "Data copied into staging table.\n",
      "Data merged into target table.\n",
      "Deleted Query/batch_1.csv from S3.\n",
      "ETL process completed for date: 2024-10-27.\n",
      "Processing data for date: 2024-10-28\n",
      "Uploaded batch_1.csv to S3.\n",
      "Data copied into staging table.\n",
      "Data merged into target table.\n",
      "Deleted Query/batch_1.csv from S3.\n",
      "ETL process completed for date: 2024-10-28.\n",
      "Processing data for date: 2024-10-29\n",
      "Uploaded batch_1.csv to S3.\n",
      "Data copied into staging table.\n",
      "Data merged into target table.\n",
      "Deleted Query/batch_1.csv from S3.\n",
      "ETL process completed for date: 2024-10-29.\n",
      "Processing data for date: 2024-10-30\n",
      "Uploaded batch_1.csv to S3.\n",
      "Data copied into staging table.\n",
      "Data merged into target table.\n",
      "Deleted Query/batch_1.csv from S3.\n",
      "ETL process completed for date: 2024-10-30.\n",
      "Processing data for date: 2024-10-31\n",
      "Uploaded batch_1.csv to S3.\n",
      "Data copied into staging table.\n",
      "Data merged into target table.\n",
      "Deleted Query/batch_1.csv from S3.\n",
      "ETL process completed for date: 2024-10-31.\n",
      "Processing data for date: 2024-11-01\n",
      "Uploaded batch_1.csv to S3.\n",
      "Data copied into staging table.\n",
      "Data merged into target table.\n",
      "Deleted Query/batch_1.csv from S3.\n",
      "ETL process completed for date: 2024-11-01.\n",
      "Processing data for date: 2024-11-02\n",
      "Uploaded batch_1.csv to S3.\n",
      "Data copied into staging table.\n",
      "Data merged into target table.\n",
      "Deleted Query/batch_1.csv from S3.\n",
      "ETL process completed for date: 2024-11-02.\n",
      "Processing data for date: 2024-11-03\n",
      "Uploaded batch_1.csv to S3.\n",
      "Data copied into staging table.\n",
      "Data merged into target table.\n",
      "Deleted Query/batch_1.csv from S3.\n",
      "ETL process completed for date: 2024-11-03.\n",
      "Processing data for date: 2024-11-04\n",
      "Uploaded batch_1.csv to S3.\n",
      "Data copied into staging table.\n",
      "Data merged into target table.\n",
      "Deleted Query/batch_1.csv from S3.\n",
      "ETL process completed for date: 2024-11-04.\n",
      "Processing data for date: 2024-11-05\n",
      "Uploaded batch_1.csv to S3.\n",
      "Data copied into staging table.\n",
      "Data merged into target table.\n",
      "Deleted Query/batch_1.csv from S3.\n",
      "ETL process completed for date: 2024-11-05.\n",
      "Processing data for date: 2024-11-06\n",
      "Uploaded batch_1.csv to S3.\n",
      "Data copied into staging table.\n",
      "Data merged into target table.\n",
      "Deleted Query/batch_1.csv from S3.\n",
      "ETL process completed for date: 2024-11-06.\n",
      "Processing data for date: 2024-11-07\n",
      "Uploaded batch_1.csv to S3.\n",
      "Data copied into staging table.\n",
      "Data merged into target table.\n",
      "Deleted Query/batch_1.csv from S3.\n",
      "ETL process completed for date: 2024-11-07.\n",
      "Processing data for date: 2024-11-08\n",
      "Uploaded batch_1.csv to S3.\n",
      "Data copied into staging table.\n",
      "Data merged into target table.\n",
      "Deleted Query/batch_1.csv from S3.\n",
      "ETL process completed for date: 2024-11-08.\n",
      "Processing data for date: 2024-11-09\n",
      "ETL process failed: <HttpError 500 when requesting https://searchconsole.googleapis.com/webmasters/v3/sites/https%3A%2F%2Foldcobblersfarm.com%2F/searchAnalytics/query?alt=json returned \"Internal error encountered.\">\n"
     ]
    }
   ],
   "source": [
    "def run_etl(days_back=60):\n",
    "    service = auth_search_console()\n",
    "    conn = connect_snowflake()\n",
    "    \n",
    "    try:\n",
    "        # Iterate over each day within the range\n",
    "        for i in range(days_back, 0, -1):\n",
    "            start_date = (datetime.now() - timedelta(days=i)).strftime('%Y-%m-%d')\n",
    "            end_date = start_date  # Same day to get data for each day individually\n",
    "            \n",
    "            print(f\"Processing data for date: {start_date}\")\n",
    "            \n",
    "            # Fetch, transform, and upload data\n",
    "            df = fetch_data_from_gsc(service, start_date, end_date)\n",
    "            df['client'] = client\n",
    "            #df['timestamp'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            df['COMPANY_ID'] = 1\n",
    "\n",
    "            if not df.empty:\n",
    "                upload_to_s3(df)\n",
    "                load_to_snowflake(conn)\n",
    "                clear_s3_folder()\n",
    "                print(f\"ETL process completed for date: {start_date}.\")\n",
    "            else:\n",
    "                print(f\"No data to process for date: {start_date}.\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"ETL process failed: {e}\")\n",
    "    finally:\n",
    "        conn.close()\n",
    "\n",
    "# Trigger ETL for the last 8 days\n",
    "run_etl(days_back=60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_etl():\n",
    "    service = auth_search_console()\n",
    "    conn = connect_snowflake()\n",
    "    try:\n",
    "        start_date = (datetime.now() - timedelta(days=8)).strftime('%Y-%m-%d')\n",
    "        end_date = (datetime.now() - timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "\n",
    "        # Fetch, transform, and upload data\n",
    "        df = fetch_data_from_gsc(service, start_date, end_date)\n",
    "        df['client'] = client\n",
    "        df['timestamp'] = ''\n",
    "        if not df.empty:\n",
    "            upload_to_s3(df)\n",
    "            load_to_snowflake(conn)\n",
    "            clear_s3_folder()\n",
    "            print(\"ETL process completed successfully.\")\n",
    "        else:\n",
    "            print(\"ETL process completed with no data to process.\")\n",
    "    except Exception as e:\n",
    "        print(f\"ETL process failed: {e}\")\n",
    "    finally:\n",
    "        conn.close()\n",
    "\n",
    "# Trigger ETL\n",
    "run_etl()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
