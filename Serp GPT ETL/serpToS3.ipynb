{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-12\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime\n",
    "from snowflake import connector\n",
    "from boto3 import client\n",
    "from pandas import json_normalize\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import requests\n",
    "import boto3\n",
    "import pandas as pd\n",
    "from io import BytesIO\n",
    "from pandas import json_normalize\n",
    "from datetime import date\n",
    "\n",
    "current_date = date.today()\n",
    "print(current_date)\n",
    "\n",
    "\n",
    "load_dotenv(override=True)  # This will reload and override existing variables\n",
    "my_var = os.getenv(\"MY_VARIABLE\")\n",
    "print(my_var)\n",
    "load_dotenv()\n",
    "snowflake_user = os.getenv(\"SNOWFLAKE_USER\")\n",
    "snowflake_password = os.getenv(\"SNOWFLAKE_PASSWORD\")\n",
    "snowflake_account = os.getenv(\"SNOWFLAKE_ACCOUNT\")\n",
    "snowflake_warehouse = os.getenv(\"SNOWFLAKE_WAREHOUSE\")\n",
    "snowflake_database = os.getenv(\"SNOWFLAKE_DATABASE\")\n",
    "snowflake_schema_raw = os.getenv(\"SNOWFLAKE_SCHEMA_RAW\")\n",
    "snowflake_schema_stage = os.getenv(\"SNOWFLAKE_SCHEMA_STAGE\")\n",
    "snowflake_schema_raw_data= os.getenv(\"SNOWFLAKE_SCHEMA_RAW_DATA\")\n",
    "aws_access_key_id = os.getenv(\"AWS_ACCESS_KEY_ID\")\n",
    "aws_secret_access_key = os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    "bucket = os.getenv(\"BUCKET\")\n",
    "folder = os.getenv(\"FOLDER\")\n",
    "api_token = os.getenv(\"SERPSTAT_API_TOKEN\")\n",
    "\n",
    "client_name = 'ocf'\n",
    "\n",
    "s3 = client(\"s3\", aws_access_key_id=aws_access_key_id, aws_secret_access_key=aws_secret_access_key)\n",
    "\n",
    "def get_snowflake_connection(schema):\n",
    "    return connector.connect(\n",
    "        user=snowflake_user,\n",
    "        password=snowflake_password,\n",
    "        account=snowflake_account,\n",
    "        warehouse=snowflake_warehouse,\n",
    "        database=snowflake_database,\n",
    "        schema=schema,\n",
    "        role=\"ACCOUNTADMIN\"\n",
    "    )\n",
    "\n",
    "def fetch_top_keywords(keywordCount):\n",
    "    query = f\"\"\"\n",
    "    WITH CTE AS (\n",
    "    SELECT \n",
    "        QUERY, \n",
    "        COUNT(1) AS CNT, \n",
    "        SUM(IMPRESSIONS) AS IMPRESSIONS,\n",
    "        ROW_NUMBER() OVER (PARTITION BY QUERY ORDER BY SUM(IMPRESSIONS) DESC) AS rank\n",
    "    FROM GOOGLE_BIGQUERY.SEARCH_CONSOLE_RAW.QUERY\n",
    "    GROUP BY QUERY\n",
    "        )\n",
    "        SELECT QUERY, CNT, IMPRESSIONS , rank\n",
    "        FROM CTE\n",
    "       --- WHERE IMPRESSIONS > 10\n",
    "        ORDER BY IMPRESSIONS DESC \n",
    "        LIMIT {keywordCount}\n",
    "        ;\n",
    "    \"\"\"\n",
    "    conn = get_snowflake_connection(snowflake_schema_raw)\n",
    "    try:\n",
    "        cur = conn.cursor()\n",
    "        cur.execute(query)\n",
    "        results = cur.fetchall()\n",
    "        column_names = [desc[0] for desc in cur.description]\n",
    "        df = pd.DataFrame(results, columns=column_names)\n",
    "    finally:\n",
    "        cur.close()\n",
    "        conn.close()\n",
    "    return df\n",
    "\n",
    "def send_api_requests(keywordSearch ,  avoidedWords , numOfPages , sizOfPage , se , api_token):\n",
    "    url = f\"https://api.serpstat.com/v4/?token={api_token}\"\n",
    "    api_requests = [\n",
    "        {\n",
    "            \"description\": \"getKeywords\",\n",
    "            \"method\": \"SerpstatKeywordProcedure.getKeywords\",\n",
    "                      \"params\": {\n",
    "                                    \"keyword\": keywordSearch,  # Replace with actual keyword\n",
    "                                    \"se\": se,\n",
    "                                    \"minusKeywords\": avoidedWords,\n",
    "                                    \"filters\": {\n",
    "                                        \"cost_from\": 1,\n",
    "                                        \"cost_to\": 10,\n",
    "                                        \"region_queries_count_from\": 300,\n",
    "                                        \"region_queries_count_to\": 2000\n",
    "                                    },\n",
    "                                    \"page\": numOfPages,\n",
    "                                    \"size\": sizOfPage}\n",
    "                                    ,\n",
    "            \"result_path\": \"result['result']['data']\",\n",
    "            \"snowflake_table\": \"SERPSTAT.KEYWORD_RAW_DATA.GETKEYWORDS\",\n",
    "            \"stage_table_shortname\": \"GETKEYWORDS_STG\",\n",
    "        },\n",
    "        {\n",
    "            \"description\": \"getSuggestions\",\n",
    "            \"method\": \"SerpstatKeywordProcedure.getSuggestions\",\n",
    "            \"params\": {\n",
    "                        \"keyword\": keywordSearch,  # Replace with actual keyword\n",
    "                        \"se\": se,\n",
    "                        \"page\": numOfPages,\n",
    "                        \"size\": sizOfPage,\n",
    "                                    },\n",
    "            \"result_path\": \"result['result']['data']\",\n",
    "            \"snowflake_table\": \"SERPSTAT.KEYWORD_RAW_DATA.GETSUGGESTIONS\",\n",
    "            \"stage_table_shortname\": \"GETSUGGESTIONS_STG\",\n",
    "        },\n",
    "        {\n",
    "            \"description\": \"getRelatedKeywords\",\n",
    "            \"method\": \"SerpstatKeywordProcedure.getRelatedKeywords\",\n",
    "            \"params\": {\n",
    "                    \"keyword\": keywordSearch,  # Replace with actual keyword\n",
    "                    \"se\": se,\n",
    "                    \"filters\": {\n",
    "                        \"cost_from\": 0,\n",
    "                        \"cost_to\": 5,\n",
    "                        \"difficulty_from\": 5,\n",
    "                        \"difficulty_to\": 30\n",
    "                    },\n",
    "                    \"page\": numOfPages,\n",
    "                    \"size\": sizOfPage\n",
    "                                },\n",
    "            \"result_path\": \"result['result']['data']\",\n",
    "            \"snowflake_table\": \"SERPSTAT.KEYWORD_RAW_DATA.GETRELATEDKEYWORDS\",\n",
    "            \"stage_table_shortname\": \"GETRELATEDKEYWORDS_STG\",\n",
    "        }\n",
    "    ]\n",
    "    return api_requests\n",
    "\n",
    "# Upload DataFrame to S3 and return path\n",
    "def upload_to_s3(df, description, index):\n",
    "    file_name = f\"{description}/csv_{index}.csv\"\n",
    "    df.to_csv(file_name, index=False)\n",
    "    s3.upload_file(file_name, bucket, f\"{folder}/{file_name}\")\n",
    "    os.remove(file_name)\n",
    "    return f\"{folder}/{file_name}\"\n",
    "\n",
    "# Process to Snowflake and track\n",
    "def process_to_snowflake(conn, s3_path, description):\n",
    "    copy_command = f\"\"\"\n",
    "    COPY INTO {description}\n",
    "    FROM 's3://{bucket}/{s3_path}'\n",
    "    CREDENTIALS = (AWS_KEY_ID='{aws_access_key_id}' AWS_SECRET_KEY='{aws_secret_access_key}')\n",
    "    FILE_FORMAT = (TYPE = 'CSV' FIELD_OPTIONALLY_ENCLOSED_BY = '\"' SKIP_HEADER=1)\n",
    "    ON_ERROR='CONTINUE';\n",
    "    \"\"\"\n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(copy_command)\n",
    "        print(f\"Data successfully copied to {description}\")\n",
    "\n",
    "\n",
    "\n",
    "def upload_df_to_s3(df, description):\n",
    "    csv_buffer = BytesIO()\n",
    "    df.to_csv(csv_buffer, index=False)\n",
    "    csv_buffer.seek(0)\n",
    "    file_name = f\"{description}/data.csv\"\n",
    "    s3_path = f\"{folder}/{file_name}\"\n",
    "    s3.put_object(Bucket=bucket, Key=s3_path, Body=csv_buffer.getvalue())\n",
    "    \n",
    "    print(f\"Data uploaded to s3://{bucket}/{s3_path}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "url = f\"https://api.serpstat.com/v4/?token={api_token}\"\n",
    "\n",
    "top_keywords = fetch_top_keywords(1000)\n",
    "\n",
    "size = 100 \n",
    "avoidedWords = ['kellogs', 'corn flakes']\n",
    "numOfPages = 5\n",
    "sizOfPage = 20\n",
    "se = 'g_us'\n",
    "\n",
    "relatedKeyWordList = []\n",
    "getSuggestionsList = []\n",
    "getKeywordsList = []\n",
    "\n",
    "# Loop through keywords and send API requests\n",
    "for keyword in top_keywords['QUERY'][:1000]:\n",
    "    for index, api_call in enumerate(send_api_requests(keyword,  avoidedWords, numOfPages, sizOfPage, se, api_token), 1):\n",
    "        result_path = api_call['result_path']  # Extract result_path\n",
    "        description = api_call['description']\n",
    "        method = api_call['method']\n",
    "        params = api_call['params']\n",
    "\n",
    "        try:\n",
    "            response = requests.post(\n",
    "                url,\n",
    "                headers={\"Authorization\": f\"Bearer {api_token}\"},\n",
    "                json={\"id\": 1, \"method\": method, \"params\": params}\n",
    "            )\n",
    "            response.raise_for_status()  # Check if the request was successful\n",
    "            if description == 'getRelatedKeywords':\n",
    "                try:\n",
    "                    data = response.json()['result']['data']\n",
    "                    data = json_normalize(data)\n",
    "                    data['keyword_searched'] = keyword\n",
    "                    relatedKeyWordList.append(data)\n",
    "                except (KeyError, ValueError) as e:\n",
    "                    print(f\"Error processing 'getRelatedKeywords' for {keyword}: {e}\")\n",
    "\n",
    "            elif description == 'getSuggestions':\n",
    "                try:\n",
    "                    data = response.json()['result']['data']\n",
    "                    data = json_normalize(data)\n",
    "                    data['keyword_searched'] = keyword\n",
    "                    getSuggestionsList.append(data)\n",
    "                except (KeyError, ValueError) as e:\n",
    "                    print(f\"Error processing 'getSuggestions' for {keyword}: {e}\")\n",
    "\n",
    "            elif description == 'getKeywords':\n",
    "                try:\n",
    "                    data = response.json()['result']['data']\n",
    "                    data = json_normalize(data)\n",
    "                    data['keyword_searched'] = keyword\n",
    "                    getKeywordsList.append(data)\n",
    "                except (KeyError, ValueError) as e:\n",
    "                    print(f\"Error processing 'getKeywords' for {keyword}: {e}\")\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Request failed for {keyword} with method {method}: {e}\")\n",
    "        except (KeyError, ValueError) as e:\n",
    "            print(f\"Unexpected response format for {keyword} with method {method}: {e}\")\n",
    "\n",
    "relatedKeyWorddf = pd.concat(relatedKeyWordList)\n",
    "getSuggestionsListdf = pd.concat(getSuggestionsList)\n",
    "getKeywordsListdf = pd.concat(getKeywordsList)\n",
    "\n",
    "getSuggestionsListdf = getSuggestionsListdf.drop(['geo_names'], axis=1)\n",
    "relatedKeyWorddf = relatedKeyWorddf.drop(['geo_names','types'], axis=1)\n",
    "getKeywordsListdf = getKeywordsListdf.drop(['geo_names','types','social_domains'], axis=1)\n",
    "\n",
    "getKeywordsListdf = getKeywordsListdf.drop_duplicates()\n",
    "relatedKeyWorddf = relatedKeyWorddf.drop_duplicates()\n",
    "getSuggestionsListdf = getSuggestionsListdf.drop_duplicates()\n",
    "\n",
    "\n",
    "\n",
    "relatedKeyWorddf = pd.concat(relatedKeyWordList).drop(['geo_names', 'types'], axis=1).drop_duplicates()\n",
    "getSuggestionsListdf = pd.concat(getSuggestionsList).drop(['geo_names'], axis=1).drop_duplicates()\n",
    "getKeywordsListdf = pd.concat(getKeywordsList).drop(['geo_names', 'types', 'social_domains'], axis=1).drop_duplicates()\n",
    "\n",
    "relatedKeyWorddf['client'] = client_name \n",
    "getSuggestionsListdf['client'] = client_name \n",
    "getKeywordsListdf['client'] = client_name \n",
    "\n",
    "relatedKeyWorddf['load_timestamp'] = ''\n",
    "getSuggestionsListdf['load_timestamp'] = ''\n",
    "getKeywordsListdf['load_timestamp'] = ''\n",
    "\n",
    "relatedKeyWorddf['timestamp'] = current_date\n",
    "getSuggestionsListdf['timestamp'] = current_date\n",
    "getKeywordsListdf['timestamp'] = current_date\n",
    "\n",
    "\n",
    "dataframes = [\n",
    "    (relatedKeyWorddf, 'related_keywords'),\n",
    "    (getSuggestionsListdf, 'suggestions'),\n",
    "    (getKeywordsListdf, 'keywords')\n",
    "]\n",
    "for df, description in dataframes:\n",
    "    upload_df_to_s3(df, description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# S3 to Snowflake Staging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime\n",
    "from snowflake import connector\n",
    "from boto3 import client\n",
    "from pandas import json_normalize\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import requests\n",
    "import boto3\n",
    "import pandas as pd\n",
    "from io import BytesIO\n",
    "from pandas import json_normalize\n",
    "\n",
    "client = 'ocf'\n",
    "load_dotenv(override=True)  # This will reload and override existing variables\n",
    "my_var = os.getenv(\"MY_VARIABLE\")\n",
    "print(my_var)\n",
    "load_dotenv()\n",
    "snowflake_user = os.getenv(\"SNOWFLAKE_USER\")\n",
    "snowflake_password = os.getenv(\"SNOWFLAKE_PASSWORD\")\n",
    "snowflake_account = os.getenv(\"SNOWFLAKE_ACCOUNT\")\n",
    "snowflake_warehouse = os.getenv(\"SNOWFLAKE_WAREHOUSE\")\n",
    "snowflake_database = os.getenv(\"SNOWFLAKE_DATABASE\")\n",
    "snowflake_schema_raw = os.getenv(\"SNOWFLAKE_SCHEMA_RAW\")\n",
    "snowflake_schema_stage = os.getenv(\"SNOWFLAKE_SCHEMA_STAGE\")\n",
    "snowflake_schema_raw_data= os.getenv(\"SNOWFLAKE_SCHEMA_RAW_DATA\")\n",
    "aws_access_key_id = os.getenv(\"AWS_ACCESS_KEY_ID\")\n",
    "aws_secret_access_key = os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    "bucket = os.getenv(\"BUCKET\")\n",
    "folder = os.getenv(\"FOLDER\")\n",
    "api_token = os.getenv(\"SERPSTAT_API_TOKEN\")\n",
    "\n",
    "\n",
    "\n",
    "def get_snowflake_connection(schema):\n",
    "    return connector.connect(\n",
    "        user=snowflake_user,\n",
    "        password=snowflake_password,\n",
    "        account=snowflake_account,\n",
    "        warehouse=snowflake_warehouse,\n",
    "        database=snowflake_database,\n",
    "        schema=schema,\n",
    "        role=\"ACCOUNTADMIN\"\n",
    "    )\n",
    "\n",
    "def snowflakeInformation():\n",
    "    api_requests = [\n",
    "        {\n",
    "            \"blob_path\": \"s3://serp-api-blob/Query/keywords/data.csv\",\n",
    "            \"snowflake_table\": \"SERPSTAT.KEYWORD_RAW_DATA.GETKEYWORDS\",\n",
    "            \"stage_table_shortname\": \"GETKEYWORDS_STG\",\n",
    "        },\n",
    "        {\n",
    "            \"blob_path\": \"s3://serp-api-blob/Query/suggestions/data.csv\",\n",
    "            \"snowflake_table\": \"SERPSTAT.KEYWORD_RAW_DATA.GETSUGGESTIONS\",\n",
    "            \"stage_table_shortname\": \"GETSUGGESTIONS_STG\",\n",
    "        },\n",
    "        {\n",
    "            \"blob_path\": \"s3://serp-api-blob/Query/related_keywords/data.csv\",\n",
    "            \"snowflake_table\": \"SERPSTAT.KEYWORD_RAW_DATA.GETRELATEDKEYWORDS\",\n",
    "            \"stage_table_shortname\": \"GETRELATEDKEYWORDS_STG\",\n",
    "        }\n",
    "    ]\n",
    "    return api_requests\n",
    "\n",
    "def execute_snowflake_copy(conn):\n",
    "    for api_call in snowflakeInformation():\n",
    "        blob_path = api_call['blob_path']\n",
    "        stage_table = api_call['stage_table_shortname']\n",
    "        \n",
    "        # Build the COPY command\n",
    "        copy_command = f\"\"\"\n",
    "        COPY INTO {stage_table}\n",
    "        FROM '{blob_path}'\n",
    "        CREDENTIALS = (AWS_KEY_ID='{aws_access_key_id}' AWS_SECRET_KEY='{aws_secret_access_key}')\n",
    "        FILE_FORMAT = (TYPE = 'CSV' FIELD_OPTIONALLY_ENCLOSED_BY = '\"' SKIP_HEADER=1)\n",
    "        ON_ERROR='CONTINUE';\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with conn.cursor() as cur:\n",
    "                cur.execute(copy_command)\n",
    "                print(f\"Data successfully copied to {stage_table}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to copy data to {stage_table}: {e}\")\n",
    "\n",
    "def timestamping(conn):\n",
    "    for api_call in snowflakeInformation():\n",
    "        stage_table = api_call['stage_table_shortname']\n",
    "        update_command = f\"\"\"\n",
    "        UPDATE {stage_table}\n",
    "        SET load_timestamp = CURRENT_TIMESTAMP\n",
    "        WHERE load_timestamp IS NULL;\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with conn.cursor() as cur:\n",
    "                cur.execute(update_command)\n",
    "                print(f\"Timestamp updated in {stage_table}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error updating timestamp in {stage_table}: {e}\")\n",
    "\n",
    "def merge_queries():\n",
    "    merges = [\"\"\"\n",
    "    MERGE INTO SERPSTAT.KEYWORD_RAW_DATA.GETRELATEDKEYWORDS AS target\n",
    "    USING SERPSTAT.KEYWORD_STAGE_TABLES.GETRELATEDKEYWORDS_STG AS source\n",
    "    ON target.keyword = source.keyword AND target.timestamp =  source.timestamp\n",
    "    WHEN MATCHED THEN\n",
    "        UPDATE SET\n",
    "            target.region_queries_count = source.region_queries_count,\n",
    "            target.cost = source.cost,\n",
    "            target.concurrency = source.concurrency,\n",
    "            target.right_spelling = source.right_spelling,\n",
    "            target.weight = source.weight,\n",
    "            target.difficulty = source.difficulty,\n",
    "            target.client = source.client,\n",
    "            target.load_timestamp = source.load_timestamp,\n",
    "            target.timestamp = source.timestamp,\n",
    "            target.keyword_searched = source.keyword_searched\n",
    "    -- Insert new records\n",
    "    WHEN NOT MATCHED THEN\n",
    "        INSERT (\n",
    "            keyword,\n",
    "            region_queries_count,\n",
    "            cost,\n",
    "            concurrency,\n",
    "            right_spelling,\n",
    "            weight,\n",
    "            difficulty,\n",
    "            keyword_searched ,\n",
    "            client,\n",
    "            load_timestamp,\n",
    "            timestamp\n",
    "        )\n",
    "        VALUES (\n",
    "            source.keyword,source.region_queries_count, source.cost,source.concurrency,\n",
    "            source.right_spelling, source.weight,source.difficulty,source.keyword_searched , source.client, source.load_timestamp,source.timestamp\n",
    "        );\n",
    "    \"\"\" , \n",
    "\n",
    "    \"\"\"\n",
    "    MERGE INTO SERPSTAT.KEYWORD_RAW_DATA.GETKEYWORDS target\n",
    "    USING SERPSTAT.KEYWORD_STAGE_TABLES.GETKEYWORDS_STG source\n",
    "    ON target.keyword = source.keyword AND target.timestamp =  source.timestamp\n",
    "    WHEN MATCHED THEN UPDATE SET target.cost = source.cost, target.concurrency = source.concurrency, \n",
    "    target.found_results = source.found_results, target.region_queries_count = source.region_queries_count, \n",
    "    target.region_queries_count_wide = source.region_queries_count_wide, target.right_spelling = source.right_spelling, \n",
    "    target.lang = source.lang, target.keyword_length = source.keyword_length, target.difficulty = source.difficulty, \n",
    "    target.client = source.client, target.load_timestamp = source.load_timestamp, target.timestamp = source.timestamp, target.keyword_searched = source.keyword_searched\n",
    "    WHEN NOT MATCHED THEN INSERT (keyword, cost, concurrency, found_results, region_queries_count, region_queries_count_wide, right_spelling, lang, keyword_length, difficulty,keyword_searched ,  client, load_timestamp, timestamp) \n",
    "    VALUES (source.keyword, source.cost, source.concurrency, source.found_results, source.region_queries_count, source.region_queries_count_wide, source.right_spelling, source.lang, source.keyword_length, source.difficulty,source.keyword_searched, source.client, source.load_timestamp, source.timestamp);\n",
    "    \"\"\" ,\n",
    "    \"\"\"\n",
    "            MERGE INTO SERPSTAT.KEYWORD_RAW_DATA.GETSUGGESTIONS AS target\n",
    "        USING SERPSTAT.KEYWORD_STAGE_TABLES.GETSUGGESTIONS_STG AS source\n",
    "        ON target.keyword = source.keyword \n",
    "        AND target.keyword_searched = source.keyword_searched \n",
    "        AND target.timestamp = source.timestamp AND t\n",
    "        arget.timestamp =  source.timestamp\n",
    "        WHEN MATCHED THEN \n",
    "            UPDATE SET \n",
    "                target.client = source.client, \n",
    "                target.timestamp = source.timestamp, \n",
    "                target.load_timestamp = source.load_timestamp\n",
    "        WHEN NOT MATCHED THEN \n",
    "            INSERT (keyword, keyword_searched, client, timestamp, load_timestamp) \n",
    "            VALUES (source.keyword, source.keyword_searched, source.client, source.timestamp, source.load_timestamp);\n",
    "    \"\"\"\n",
    "    ]\n",
    "    return merges\n",
    "\n",
    "def delete_file_from_s3(path):\n",
    "    try:\n",
    "        s3.delete_object(Bucket=bucket, Key=path)\n",
    "        print(f\"File deleted from s3://{path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error deleting file: {e}\")\n",
    "\n",
    "\n",
    "def merging_data(conn):\n",
    "    for merge in merge_queries():\n",
    "        try:\n",
    "            with conn.cursor() as cur:\n",
    "                cur.execute(merge)\n",
    "                cur.execute(merge)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error updating timestamp\")\n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(\"truncate SERPSTAT.KEYWORD_STAGE_TABLES.GETSUGGESTIONS_STG \")\n",
    "        cur.execute(\"truncate SERPSTAT.KEYWORD_STAGE_TABLES.GETKEYWORDS_STG \")\n",
    "        cur.execute(\"truncate SERPSTAT.KEYWORD_STAGE_TABLES.GETRELATEDKEYWORDS_STG \")\n",
    "\n",
    "execute_snowflake_copy(get_snowflake_connection(snowflake_schema_stage))\n",
    "timestamping(get_snowflake_connection(snowflake_schema_stage))\n",
    "merging_data(get_snowflake_connection(snowflake_schema_stage))\n",
    "\n",
    "for api_call in snowflakeInformation():\n",
    "    blob_path = api_call['blob_path']\n",
    "    stage_table = api_call['stage_table_shortname']\n",
    "    \n",
    "    delete_file_from_s3(blob_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blog Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('requirements.txt', 'r') as file:\n",
    "    packages = file.read().splitlines()\n",
    "\n",
    "# List of standard libraries that shouldn't be installed with pip\n",
    "standard_libs = {\"json\", \"os\", \"sys\", \"re\", \"time\"}  # add more if necessary\n",
    "for package in packages:\n",
    "    # Skip standard library modules\n",
    "    if package in standard_libs:\n",
    "        print(f\"Skipping standard library module: {package}\")\n",
    "        continue\n",
    "    try:\n",
    "        subprocess.check_call(['pip', 'install', package])\n",
    "        print(f\"Successfully installed {package}\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Failed to install {package}. Error: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime\n",
    "from snowflake import connector\n",
    "from boto3 import client\n",
    "from pandas import json_normalize\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import requests\n",
    "import boto3\n",
    "import pandas as pd\n",
    "from io import BytesIO\n",
    "from pandas import json_normalize\n",
    "import subprocess\n",
    "import openai \n",
    "\n",
    "client = 'ocf'\n",
    "load_dotenv(override=True)  # This will reload and override existing variables\n",
    "my_var = os.getenv(\"MY_VARIABLE\")\n",
    "print(my_var)\n",
    "load_dotenv()\n",
    "snowflake_user = os.getenv(\"SNOWFLAKE_USER\")\n",
    "snowflake_password = os.getenv(\"SNOWFLAKE_PASSWORD\")\n",
    "snowflake_account = os.getenv(\"SNOWFLAKE_ACCOUNT\")\n",
    "snowflake_warehouse = os.getenv(\"SNOWFLAKE_WAREHOUSE\")\n",
    "snowflake_database = os.getenv(\"SNOWFLAKE_DATABASE\")\n",
    "snowflake_schema_raw = os.getenv(\"SNOWFLAKE_SCHEMA_RAW\")\n",
    "snowflake_schema_stage = os.getenv(\"SNOWFLAKE_SCHEMA_STAGE\")\n",
    "snowflake_schema_raw_data= os.getenv(\"SNOWFLAKE_SCHEMA_RAW_DATA\")\n",
    "aws_access_key_id = os.getenv(\"AWS_ACCESS_KEY_ID\")\n",
    "aws_secret_access_key = os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    "bucket = os.getenv(\"BUCKET\")\n",
    "folder = os.getenv(\"FOLDER\")\n",
    "api_token = os.getenv(\"SERPSTAT_API_TOKEN\")\n",
    "#openAiKey = 'sk-'\n",
    "#os.getenv(\"SERPSTAT_API_TOKEN\")\n",
    "#openai.api_key = 'sk-'\n",
    "\n",
    "\n",
    "def get_snowflake_connection(schema):\n",
    "    return connector.connect(\n",
    "        user=snowflake_user,\n",
    "        password=snowflake_password,\n",
    "        account=snowflake_account,\n",
    "        warehouse=snowflake_warehouse,\n",
    "        database=snowflake_database,\n",
    "        schema=schema,\n",
    "        role=\"ACCOUNTADMIN\"\n",
    "    )\n",
    "\n",
    "current_date = datetime.now().strftime('%Y-%m-%d')\n",
    "\n",
    "url = f\"https://api.serpstat.com/v4/?token={api_token}\"\n",
    "\n",
    "\n",
    "staging_table = \"CONTENT.STAGING_TABLES.PRODUCT_CONTENTS\"\n",
    "batch_size = 100000\n",
    "snowflake_table = \"CONTENT.BLOG_POSTS.PRODUCT_CONTENTS\"\n",
    "company_id= 1\n",
    "\n",
    "def query(conn,limit):\n",
    "    cur = conn.cursor()\n",
    "    query = f\"\"\" \n",
    "       select QUERY, \n",
    "            KW.KEYWORD AS KW_KEYWORD ,\n",
    "            R.KEYWORD AS R_KEYWORD, \n",
    "            GS.KEYWORD_SEARCHED ,\n",
    "            SUM(IMPRESSIONS) AS IMPRESSIONS, \n",
    "            SUM(CLICKS) AS CLICKS \n",
    "            from GOOGLE_BIGQUERY.SEARCH_CONSOLE_RAW.QUERY Q \n",
    "            LEFT JOIN  SERPSTAT.KEYWORD_RAW_DATA.GETKEYWORDS KW ON Q.QUERY = KW.KEYWORD_SEARCHED\n",
    "            LEFT JOIN SERPSTAT.KEYWORD_RAW_DATA.GETRELATEDKEYWORDS R ON Q.QUERY =  R.KEYWORD_SEARCHED\n",
    "            LEFT JOIN  SERPSTAT.KEYWORD_RAW_DATA.GETSUGGESTIONS GS ON Q.QUERY = GS.KEYWORD_SEARCHED\n",
    "            WHERE KW.KEYWORD IS NOT NULL AND R.KEYWORD  IS NOT NULL\n",
    "            GROUP BY 1,2,3,4\n",
    "            ORDER BY 5 DESC \n",
    "            LIMIT {limit};\n",
    "            \"\"\"  \n",
    "    cur.execute(query)\n",
    "    results = cur.fetchall()\n",
    "    column_names = [desc[0] for desc in cur.description]\n",
    "    df = pd.DataFrame(results, columns=column_names)\n",
    "    return df \n",
    "\n",
    "def generate_text(prompt, max_tokens=150):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",  # You can use \"gpt-4\" if available on your account\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        max_tokens=max_tokens,\n",
    "        n=1,\n",
    "        stop=None,\n",
    "        temperature=0.7  # Controls randomness (higher = more random)\n",
    "    )\n",
    "    return response['choices'][0]['message']['content'].strip()\n",
    "\n",
    "def generate_blog_post(product_name, product_description, suggested_keywords, query_keywords):\n",
    "    prompt = (f\"Write a detailed blog post about {product_name}. \"\n",
    "              f\"Description: {product_description}. Highlight the following: {suggested_keywords}, \"\n",
    "              f\"and target the following search queries: {query_keywords}. \"\n",
    "              \"Mention that readers can also check out our social media post for more updates.\")\n",
    "    return generate_text(prompt, max_tokens=400)\n",
    "\n",
    "\n",
    "def generate_social_post(product_name, product_description, suggested_keywords, query_keywords):\n",
    "    hashtags = f\"#{suggested_keywords.replace(' ', '')} #{query_keywords.replace(' ', '')}\"\n",
    "    \n",
    "    prompt = (f\"Write a short social media post about {product_name}. \"\n",
    "              f\"Description: {product_description}. Focus on {suggested_keywords} and use the keywords \"\n",
    "              f\"{query_keywords}. Include a call to action telling users to check out our blog post for more details. \"\n",
    "              \"Also, generate a caption and relevant hashtags.\")\n",
    "    \n",
    "    social_post = generate_text(prompt, max_tokens=100)\n",
    "    \n",
    "    # Append hashtags and call to action for blog post\n",
    "    full_social_post = f\"{social_post}\\n\\nRead more in our blog post! [Insert Blog Link Here]\\n{hashtags}\"\n",
    "    return full_social_post\n",
    "\n",
    "final_df = pd.DataFrame()\n",
    "\n",
    "counter = 0\n",
    "\n",
    "df = query(get_snowflake_connection(snowflake_schema_raw_data), 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No compatible files (CSV/Excel) found in the folder.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define your folder ID\n",
    "FOLDER_ID = '1DgNAor9qfPubo808ia_N8gW-oOhmPcUb'\n",
    "\n",
    "# Authenticate with Google Drive API\n",
    "SCOPES = ['https://www.googleapis.com/auth/drive']\n",
    "creds = service_account.Credentials.from_service_account_file('surremor.json', scopes=SCOPES)\n",
    "service = build('drive', 'v3', credentials=creds)\n",
    "\n",
    "# List all files in the folder\n",
    "def list_files_in_folder(folder_id):\n",
    "    query = f\"'{folder_id}' in parents\"\n",
    "    results = service.files().list(\n",
    "        q=query,\n",
    "        spaces='drive',\n",
    "        fields='files(id, name, mimeType)',\n",
    "    ).execute()\n",
    "    return results.get('files', [])\n",
    "\n",
    "# Download and export files as needed\n",
    "def download_or_export_file(file_id, file_name, mime_type):\n",
    "    # Handle Google Docs Editors files by exporting them in the appropriate format\n",
    "    if mime_type == 'application/vnd.google-apps.spreadsheet':\n",
    "        request = service.files().export(fileId=file_id, mimeType='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet')\n",
    "        file_name = f\"{file_name}.xlsx\"\n",
    "    elif mime_type == 'application/vnd.google-apps.document':\n",
    "        request = service.files().export(fileId=file_id, mimeType='application/pdf')\n",
    "        file_name = f\"{file_name}.pdf\"\n",
    "    else:\n",
    "        request = service.files().get_media(fileId=file_id)\n",
    "\n",
    "    # Download the file\n",
    "    fh = io.FileIO(file_name, 'wb')\n",
    "    downloader = MediaIoBaseDownload(fh, request)\n",
    "    \n",
    "    done = False\n",
    "    while not done:\n",
    "        status, done = downloader.next_chunk()\n",
    "        print(f\"Downloading {file_name}: {int(status.progress() * 100)}% complete.\")\n",
    "    \n",
    "    fh.close()\n",
    "    print(f\"Downloaded {file_name}\")\n",
    "    return file_name\n",
    "\n",
    "# Download all files in the folder, export Google Sheets to DataFrame\n",
    "def download_all_files_from_folder(folder_id):\n",
    "    files = list_files_in_folder(folder_id)\n",
    "    dataframes = []\n",
    "    \n",
    "    for file in files:\n",
    "        file_id = file['id']\n",
    "        file_name = file['name']\n",
    "        mime_type = file['mimeType']\n",
    "        \n",
    "        # Download or export the file based on type\n",
    "        downloaded_file = download_or_export_file(file_id, file_name, mime_type)\n",
    "        \n",
    "        # Load exported Google Sheets files and CSV files into DataFrames\n",
    "        if downloaded_file.endswith('.xlsx'):\n",
    "            df = pd.read_excel(downloaded_file)\n",
    "            dataframes.append(df)\n",
    "        elif downloaded_file.endswith('.csv'):\n",
    "            df = pd.read_csv(downloaded_file)\n",
    "            dataframes.append(df)\n",
    "        \n",
    "        # Optionally, delete the downloaded file after loading\n",
    "        os.remove(downloaded_file)\n",
    "    \n",
    "    # Combine all DataFrames into one if applicable\n",
    "    combined_df = pd.concat(dataframes, ignore_index=True) if dataframes else None\n",
    "    return combined_df\n",
    "\n",
    "# Run the download and export function\n",
    "combined_df = download_all_files_from_folder(FOLDER_ID)\n",
    "\n",
    "# Display the combined DataFrame if data was loaded\n",
    "if combined_df is not None:\n",
    "    print(combined_df.head())\n",
    "else:\n",
    "    print(\"No compatible files (CSV/Excel) found in the folder.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     category    product description  variant       QUERY   \n",
      "0  fertilizer  greensand  green sand        5  green sand  \\\n",
      "\n",
      "                    KW_KEYWORD           R_KEYWORD KEYWORD_SEARCHED   \n",
      "0  green sand water filtration  what is green sand       green sand  \\\n",
      "\n",
      "   IMPRESSIONS  CLICKS  similarity_score  \n",
      "0           39       0               1.0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Sample data (replace this with your actual loaded data)\n",
    "# combined_df and df are assumed to be already loaded\n",
    "\n",
    "# Step 1: Calculate the cosine similarity\n",
    "def get_best_match(combined_df, df):\n",
    "    # Vectorize the text columns\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    combined_descriptions = combined_df['description'].values\n",
    "    df_queries = df['QUERY'].values\n",
    "    \n",
    "    # Fit and transform the text data\n",
    "    combined_vec = vectorizer.fit_transform(combined_descriptions)\n",
    "    df_vec = vectorizer.transform(df_queries)\n",
    "    \n",
    "    # Calculate cosine similarity\n",
    "    similarity_matrix = cosine_similarity(combined_vec, df_vec)\n",
    "    \n",
    "    # Find the index of the most similar row in df for each row in combined_df\n",
    "    best_matches = similarity_matrix.argmax(axis=1)\n",
    "    best_match_scores = similarity_matrix.max(axis=1)\n",
    "    \n",
    "    # Get the most similar row in df for each row in combined_df\n",
    "    best_match_df = df.iloc[best_matches].reset_index(drop=True)\n",
    "    best_match_df['similarity_score'] = best_match_scores  # Add similarity score for reference\n",
    "    \n",
    "    # Concatenate with combined_df for a left join effect\n",
    "    merged_df = pd.concat([combined_df.reset_index(drop=True), best_match_df], axis=1)\n",
    "    \n",
    "    return merged_df\n",
    "\n",
    "# Perform the left join with cosine similarity matching\n",
    "merged_df = get_best_match(combined_df, df)\n",
    "\n",
    "# Display the result\n",
    "print(merged_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Query                        KW_KEYWORD           R_KEYWORD   \n",
      "0   green sand       green sand water filtration  what is green sand  \\\n",
      "1  corn gluten  gluten free corn flakes kelloggs   weeds corn gluten   \n",
      "\n",
      "  KEYWORD_SEARCHED                                  Blog Post Content  \n",
      "0       green sand  Title: The Magic of Green Sand: A Natural Powe...  \n",
      "1      corn gluten  Title: The Power of Corn Gluten: From Kellogg'...  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import openai\n",
    "\n",
    "# Function to check for uniqueness using cosine similarity\n",
    "def is_unique(new_content, previous_posts, similarity_threshold=0.7):\n",
    "    if not previous_posts:\n",
    "        return True  # If there are no previous posts, the content is unique by default\n",
    "\n",
    "    # Vectorize the previous posts and the new content together\n",
    "    vectorizer = TfidfVectorizer().fit_transform(previous_posts + [new_content])\n",
    "    cosine_matrix = cosine_similarity(vectorizer[-1], vectorizer[:-1])\n",
    "    \n",
    "    # Find the maximum similarity score\n",
    "    max_similarity = cosine_matrix.max()\n",
    "    \n",
    "    # If max similarity is below the threshold, content is unique\n",
    "    return max_similarity < similarity_threshold\n",
    "\n",
    "# Function to generate unique blog content\n",
    "def generate_unique_blog_content(query, kw_keyword, r_keyword, keyword_searched, previous_posts):\n",
    "    attempt = 0\n",
    "    max_attempts = 3\n",
    "    unique_content = None\n",
    "\n",
    "    while attempt < max_attempts:\n",
    "        prompt = (f\"Write a unique blog post about '{query}', using the following keywords naturally in the content:\\n\\n\"\n",
    "                  f\"KW_KEYWORD: {kw_keyword}\\n\"\n",
    "                  f\"R_KEYWORD: {r_keyword}\\n\"\n",
    "                  f\"KEYWORD_SEARCHED: {keyword_searched}\\n\\n\"\n",
    "                  \"Create a unique perspective with new examples, tips, or user stories that differ from similar topics.\")\n",
    "        \n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-4\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.7\n",
    "        )\n",
    "        generated_content = response.choices[0].message['content']\n",
    "\n",
    "        # Check uniqueness against previous posts using cosine similarity\n",
    "        if is_unique(generated_content, previous_posts):\n",
    "            unique_content = generated_content\n",
    "            break\n",
    "\n",
    "        attempt += 1\n",
    "    return unique_content\n",
    "\n",
    "# List to store each post's data for the DataFrame\n",
    "output_data = []\n",
    "previous_posts = []  # Stores content for similarity checking\n",
    "\n",
    "# Generate content for each row in the DataFrame\n",
    "for _, row in df.iterrows():\n",
    "    blog_post = generate_unique_blog_content(\n",
    "        row[\"QUERY\"],\n",
    "        row[\"KW_KEYWORD\"],\n",
    "        row[\"R_KEYWORD\"],\n",
    "        row[\"KEYWORD_SEARCHED\"],\n",
    "        previous_posts\n",
    "    )\n",
    "    output_data.append({\n",
    "        \"Query\": row[\"QUERY\"],\n",
    "        \"KW_KEYWORD\": row[\"KW_KEYWORD\"],\n",
    "        \"R_KEYWORD\": row[\"R_KEYWORD\"],\n",
    "        \"KEYWORD_SEARCHED\": row[\"KEYWORD_SEARCHED\"],\n",
    "        \"Blog Post Content\": blog_post\n",
    "    })\n",
    "    if blog_post:  # Add only if content was generated\n",
    "        previous_posts.append(blog_post)  # Add generated post to list for future similarity checks\n",
    "# Create DataFrame from output data\n",
    "blog_posts_df = pd.DataFrame(output_data)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(blog_posts_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DRIVE UPLOAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File uploaded successfully with ID: 1BY8e7iW0_tjJZPd80VGQNLT4aa8fwXse\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from google.oauth2 import service_account\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.http import MediaFileUpload\n",
    "from datetime import datetime\n",
    "\n",
    "# Get the current date and format it\n",
    "current_date = datetime.now().strftime(\"%Y%m%d\")\n",
    "\n",
    "FOLDER_ID = '1C8Aym9Nh9rxeFdje6-7IK98pdJZtPAWL'\n",
    "csv_path = f'blog_posts_{current_date}.csv'\n",
    "\n",
    "# Save the DataFrame as CSV\n",
    "blog_posts_df.to_csv(csv_path, index=False)\n",
    "\n",
    "# Authenticate and create the Google Drive API client\n",
    "SCOPES = ['https://www.googleapis.com/auth/drive.file']\n",
    "creds = service_account.Credentials.from_service_account_file('surremor.json', scopes=SCOPES)\n",
    "service = build('drive', 'v3', credentials=creds)\n",
    "\n",
    "# Function to upload CSV to Google Drive\n",
    "def upload_csv_to_drive(file_path, folder_id):\n",
    "    file_metadata = {\n",
    "        'name': file_path.split('/')[-1],  # Name in Drive\n",
    "        'parents': [folder_id]  # Target folder ID in Drive\n",
    "    }\n",
    "    \n",
    "    # Set the MIME type to 'text/csv' to ensure it stays as a CSV\n",
    "    media = MediaFileUpload(file_path, mimetype='text/csv', resumable=True)\n",
    "    \n",
    "    uploaded_file = service.files().create(\n",
    "        body=file_metadata,\n",
    "        media_body=media,\n",
    "        fields='id'\n",
    "    ).execute()\n",
    "    print(f\"File uploaded successfully with ID: {uploaded_file.get('id')}\")\n",
    "\n",
    "# Run the upload function\n",
    "upload_csv_to_drive(csv_path, FOLDER_ID)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
